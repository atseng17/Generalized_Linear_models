{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generalized Linear models\n",
    "\n",
    "OLS and GLM share nearly the same features, but differ in some aspects.\n",
    "\n",
    "In linear models we assume that the data distribution is normal distributed, which  can be written as:\n",
    "$$Y_i \\sim (\\mu_i,\\sigma^2)$$\n",
    "\n",
    "In GLMs we choose the dristribution from the exponential family, where normal distribution is one one the family members.\n",
    "$$Y_i \\sim Exponential  Family$$\n",
    "\n",
    "No matter OLS or GLM, they are both Linear predictors, which means that the predictors are the combinations of parameters. Note that all the parameters only have power 1, this is why it is called linear.\n",
    "$$\\eta_i=\\alpha+\\beta x_i+\\gamma x_i^2+...$$\n",
    "\n",
    "Link function links the predictor the the estimated value. In OLS, the value that the predictor estimates, $\\eta$, is the mean, $\\mu$. Thus,\n",
    "$$\\eta_i=\\mu_i$$\n",
    "Such that,\n",
    "$$\\mu_i=\\alpha+\\beta x_i+...$$\n",
    "\n",
    "However, in GLM, the estimated value might not me the mean, because the data distribution doe not have to be a normal distribution.\n",
    "$$\\eta_i=ln(\\mu_i)$$\n",
    "Such that,\n",
    "$$\\mu_i=e^{\\alpha+\\beta x_i+...}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After learning the difference between OLS and GLM, Now we are going to see how the procedure goes.\n",
    "\n",
    "#### For OLS(MLE)\n",
    "Obtain the likelihood \n",
    "$$ L(\\mu)=\\Pi_{i=0}^{i=n} f(y_i) $$\n",
    "\n",
    "Obtain the log-likelihood \n",
    "$$ ln(L(\\mu))=\\Sigma_{i=0}^{i=n} ln(f(y_i)) $$\n",
    "\n",
    "Use MLE to get the mean that maximizes the likehood. In other words, differentiate and set the derivative equal to 0, solve it and obtain $\\mu$.\n",
    "$$ \\frac{d}{d\\mu}ln(L(\\mu))=0$$\n",
    "\n",
    "#### For GLM\n",
    "\n",
    "Only the third step changes, where we are going to replace the mean to the estimated value that suits the distribution, look upon the previous cell. Replacing $\\mu$ to $\\mu_i=e^{\\alpha+\\beta x_i}$, we get:\n",
    "$$ \\frac{d}{d\\mu}ln(L(\\alpha, \\beta,...))=0$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source:\n",
    "    \n",
    "- https://www.youtube.com/watch?v=vpKpFMUMaVw\n",
    "- https://www.youtube.com/watch?v=xEwM1nzQckY\n",
    "- https://tsmatz.wordpress.com/2017/08/30/glm-regression-logistic-poisson-gaussian-gamma-tutorial-with-r/\n",
    "- http://www.stat.cmu.edu/~ryantibs/advmethods/notes/glm.pdf\n",
    "- https://www.statsmodels.org/dev/examples/notebooks/generated/glm.html\n",
    "- https://www.bayesianmodelsforastrophysicaldata.com/code-5-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Data\n",
    "x = np.array([13, 10, 15, 9, 18, 22, 29, 13, 17, 11, 27, 21, 16, 14, 18, 8])\n",
    "y = np.array([1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0])\n",
    "\n",
    "X = np.transpose(x) \n",
    "X = sm.add_constant(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.4998868  -0.09194708]\n",
      "[ 1.28167832 -0.07886521]\n",
      "[ 1.2860567  -0.07915161]\n",
      "[ 1.28605881 -0.07915176]\n",
      "[ 1.28605881 -0.07915176]\n",
      "[ 1.28605881 -0.07915176]\n",
      "[ 1.28605881 -0.07915176]\n",
      "[ 1.28605881 -0.07915176]\n"
     ]
    }
   ],
   "source": [
    "# Fit\n",
    "mu = (y + 0.5) / 2                                                        # initialize mu\n",
    "eta = np.log(mu/(1-mu))                                             # initialize eta with the Bernoulli link\n",
    "\n",
    "for i in range(8):\n",
    "    w = mu * (1 - mu);                                                 # variance function\n",
    "    z = eta + (y - mu)/(mu * (1 - mu))                         # working response\n",
    "    mod = sm.WLS(z, X, weights=w).fit()                 # weigthed regression\n",
    "    eta = mod.predict()                                                # linear predictor\n",
    "    mu = 1/(1 + np.exp(-eta))                                      # fitted value\n",
    "    print(mod.params)                                                # print iteration log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output\n",
    "print(mod.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write data as dictionary\n",
    "mydata = {}\n",
    "mydata['x'] = x\n",
    "mydata['y'] = y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit using glm package\n",
    "\n",
    "import statsmodels.formula.api as smf\n",
    " \n",
    "\n",
    "mylogit = smf.glm(formula='y ~ x', data=mydata, family=sm.families.Binomial())\n",
    "res = mylogit.fit()\n",
    "print(res.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P.s. What is the WLS method in the sm toolbox?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[1,2,3],[1,2,3],[4,5,6],[1,2,3],[4,5,6],[1,2,3],[1,2,3],[4,5,6],[4,5,6],[1,2,3]])\n",
    "z =[[2.00000000e+000],[2.00000000e+000],[-2.00000000e+000],[2.00000000e+000],[-2.00000000e+000], [2.00000000e+000],[2.00000000e+000],[-2.00000000e+000],[-2.00000000e+000],[2.00000000e+000]]\n",
    "\n",
    "mod_wls = sm.WLS(z, X)\n",
    "temp_g = mod_wls.fit()\n",
    "print(temp_g.params)\n",
    "print(temp_g.predict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the GLM module in stats toolbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from scipy import stats\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "data = sm.datasets.star98.load()\n",
    "data.exog = sm.add_constant(data.exog, prepend=False)\n",
    "\n",
    "print('the shape of dependent variables',data.endog.shape)\n",
    "print('the shape of independent variables',data.exog.shape)\n",
    "\n",
    "print(data.endog[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GLM with H2o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://aichamp.wordpress.com/2017/09/29/python-example-of-building-glm-gbm-and-random-forest-binomial-model-with-h2o/\n",
    "https://www.h2o.ai/wp-content/uploads/2018/01/GLM-BOOKLET.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h2o\n",
    "h2o.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = h2o.import_file(\"https://raw.githubusercontent.com/h2oai/sparkling-water/master/examples/smalldata/prostate.csv\")\n",
    "df.summary()\n",
    "df.col_names\n",
    "##\n",
    "y = 'CAPSULE'\n",
    "x = df.col_names\n",
    "x.remove(y)\n",
    "print(\"Response = \" + y)\n",
    "print(\"Pridictors = \" + str(x))\n",
    "##\n",
    "df['CAPSULE'] = df['CAPSULE'].asfactor()\n",
    "##\n",
    "df['CAPSULE'].levels()\n",
    "[['0', '1']]\n",
    "##\n",
    "train, valid, test = df.split_frame(ratios=[.8, .1])\n",
    "print(df.shape)\n",
    "print(train.shape)\n",
    "print(valid.shape)\n",
    "print(test.shape)\n",
    "##\n",
    "from h2o.estimators.glm import H2OGeneralizedLinearEstimator\n",
    "glm_logistic = H2OGeneralizedLinearEstimator(family = \"binomial\")\n",
    "glm_logistic.train(x=x, y= y, training_frame=train, validation_frame=valid, \n",
    " model_id=\"glm_logistic\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
